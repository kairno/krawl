<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdapLeR: Speeding up Inference by Adaptive Length Reduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,116.17,97.49,76.49,10.75"><forename type="first">Ali</forename><surname>Modarressi</surname></persName>
							<email>m_modarressi@comp.iust.ac.irh.mohebbi@uvt.nlmp792@cam.ac.uk</email>
						</author>
						<author>
							<persName coords="1,217.12,97.49,83.92,10.75"><forename type="first">Hosein</forename><surname>Mohebbi</surname></persName>
						</author>
						<author>
							<persName coords="1,329.26,97.49,141.58,10.75"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0" coords="1,191.60,116.05,229.32,10.37;1,166.44,134.13,279.65,10.37">
								<orgName type="department">Iran Cognitive Science and AI</orgName>
								<orgName type="institution" key="instit1">Iran University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Tilburg University</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1" coords="1,154.88,152.22,302.76,10.37">
								<orgName type="department">Tehran Institute for Advanced Studies</orgName>
								<orgName type="institution">Khatam University</orgName>
								<address>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AdapLeR: Speeding up Inference by Adaptive Length Reduction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3FE27621FA9F48910ADBD0A0FAC2D48B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-05-21T00:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[affiliation, biblStruct, figure, formula, head, note, persName, ref, s], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained language models have shown stellar performance in various downstream tasks. But, this usually comes at the cost of high latency and computation, hindering their usage in resource-limited settings. In this work, we propose a novel approach for reducing the computational cost of BERT with minimal loss in downstream performance. Our method dynamically eliminates less contributing tokens through layers, resulting in shorter lengths and consequently lower computational cost. To determine the importance of each token representation, we train a Contribution Predictor for each layer using a gradient-based saliency method. Our experiments on several diverse classification tasks show speedups up to 22x during inference time without much sacrifice in performance. We also validate the quality of the selected tokens in our method using human annotations in the ERASER benchmark. In comparison to other widely used strategies for selecting important tokens, such as saliency and attention, our proposed method has a significantly lower false positive rate in generating rationales. Our code is freely available at <ref type="url" coords="1,99.96,540.41,173.36,7.01;1,87.57,552.36,38.79,7.01" target="https://github.com/amodaresi/AdapLeR">https://github.com/amodaresi/  AdapLeR</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" coords="1,70.86,575.17,82.82,10.75">Introduction</head><p>While large-scale pre-trained language models exhibit remarkable performances on various NLP benchmarks, their excessive computational costs and high inference latency have limited their usage in resource-limited settings. In this regard, there have been various attempts at improving the efficiency of BERT-based models <ref type="bibr" coords="1,227.28,679.41,63.22,9.46;1,70.86,692.96,23.95,9.46" target="#b6">(Devlin et al., 2019)</ref>, including knowledge distilation <ref type="bibr" coords="1,254.52,692.96,34.62,9.46;1,70.86,706.51,54.87,9.46" target="#b15">(Hinton et al., 2015;</ref><ref type="bibr" coords="1,129.67,706.51,81.07,9.46" target="#b36">Sanh et al., 2019;</ref><ref type="bibr" coords="1,214.68,706.51,70.81,9.46" target="#b45">Sun et al., 2019</ref><ref type="bibr" coords="1,70.86,720.06,25.23,9.46" target="#b47">Sun et al., , 2020;;</ref><ref type="bibr" coords="1,98.82,720.06,71.41,9.46" target="#b19">Jiao et al., 2020)</ref>, quantization <ref type="bibr" coords="1,235.21,720.06,55.29,9.46;1,70.86,733.61,24.36,9.46" target="#b11">(Gong et al., 2014;</ref><ref type="bibr" coords="1,97.54,733.61,73.52,9.46" target="#b41">Shen et al., 2020;</ref><ref type="bibr" coords="1,173.40,733.61,79.62,9.46" target="#b49">Tambe et al., 2021)</ref>, weight ⋆ Equal Contribution.</p><p>† Work done as a Master's student at IUST.</p><p>pruning <ref type="bibr" coords="1,343.21,216.44,76.85,9.46" target="#b13">(Han et al., 2016;</ref><ref type="bibr" coords="1,422.78,216.44,67.65,9.46" target="#b14">He et al., 2017;</ref><ref type="bibr" coords="1,493.15,216.44,31.28,9.46;1,306.14,229.99,54.84,9.46" target="#b32">Michel et al., 2019;</ref><ref type="bibr" coords="1,364.91,229.99,79.64,9.46" target="#b37">Sanh et al., 2020)</ref>, and progressive module replacing <ref type="bibr" coords="1,387.58,243.54,72.85,9.46" target="#b58">(Xu et al., 2020)</ref>. Despite providing significant reduction in model size, these techniques are generally static at inference time, i.e., they dedicate the same amount of computation to all inputs, irrespective of their difficulty.</p><p>A number of techniques have been also proposed in order to make efficiency enhancement sensitive to inputs. Early exit mechanism <ref type="bibr" coords="1,453.00,341.14,72.79,9.46;1,306.14,354.69,29.70,9.46">(Schwartz et al., 2020b;</ref><ref type="bibr" coords="1,338.42,354.69,72.49,9.46" target="#b26">Liao et al., 2021;</ref><ref type="bibr" coords="1,413.49,354.69,68.92,9.46" target="#b56">Xin et al., 2020;</ref><ref type="bibr" coords="1,484.99,354.69,40.80,9.46;1,306.14,368.24,25.35,9.46" target="#b27">Liu et al., 2020;</ref><ref type="bibr" coords="1,336.03,368.24,77.27,9.46" target="#b57">Xin et al., 2021;</ref><ref type="bibr" coords="1,417.85,368.24,77.91,9.46" target="#b46">Sun et al., 2021;</ref><ref type="bibr" coords="1,500.28,368.24,20.76,9.46;1,306.14,381.79,80.36,9.46" target="#b10">Eyzaguirre et al., 2021</ref>) is a commonly used method in which each layer in the model is coupled with an intermediate classifier to predict the target label. At inference, a halting condition is used to determine whether the model allows an example to exit without passing through all layers. Various halting conditions have been proposed, including Shannon's entropy <ref type="bibr" coords="1,406.97,476.64,74.25,9.46" target="#b56">(Xin et al., 2020;</ref><ref type="bibr" coords="1,483.96,476.64,41.83,9.46;1,306.14,490.19,23.95,9.46" target="#b27">Liu et al., 2020)</ref>, softmax outputs with temperature calibration <ref type="bibr" coords="1,326.91,503.74,106.41,9.46">(Schwartz et al., 2020b)</ref>, trained confidence predictors <ref type="bibr" coords="1,351.01,517.29,69.71,9.46" target="#b57">(Xin et al., 2021)</ref>, or the number of agreements between predictions of intermediate classifiers <ref type="bibr" coords="1,327.66,544.39,79.54,9.46" target="#b64">(Zhou et al., 2020)</ref>.</p><p>Most of these input-adaptive techniques compress the model from the depth perspective (i.e., reducing the number of involved encoder layers). However, one can view compression from the width perspective <ref type="bibr" coords="1,389.63,614.89,90.20,9.46" target="#b12">(Goyal et al., 2020;</ref><ref type="bibr" coords="1,483.99,614.89,41.80,9.46;1,306.14,628.44,23.95,9.46" target="#b59">Ye et al., 2021)</ref>, i.e., reducing the length of hidden states. <ref type="bibr" coords="1,305.78,641.99,85.40,9.46" target="#b9">(Ethayarajh, 2019;</ref><ref type="bibr" coords="1,396.54,641.99,124.99,9.46" target="#b22">Klafka and Ettinger, 2020)</ref>. This is particularly promising as recent analytical studies showed that there are redundant encoded information in token representations <ref type="bibr" coords="1,471.36,682.64,53.07,9.46;1,306.14,696.19,67.29,9.46" target="#b22">(Klafka and Ettinger, 2020;</ref><ref type="bibr" coords="1,376.73,696.19,78.24,9.46" target="#b9">Ethayarajh, 2019)</ref>. Among these redundancies, some tokens carry more task-specific information than others <ref type="bibr" coords="1,418.29,723.29,102.70,9.46" target="#b33">(Mohebbi et al., 2021)</ref>, suggesting that only these tokens could be considered through the model. Moreover, in contrast to layer-wise pruning, token-level pruning does not come at the cost of reducing model's capacity in complex reasoning <ref type="bibr" coords="2,158.87,88.25,82.81,9.46" target="#b36">(Sanh et al., 2019;</ref><ref type="bibr" coords="2,244.97,88.25,45.52,9.46;2,70.86,101.80,23.01,9.46" target="#b45">Sun et al., 2019)</ref>. PoWER-BERT <ref type="bibr" coords="2,169.15,101.80,81.91,9.46" target="#b12">(Goyal et al., 2020)</ref> is one of the first such techniques which reduces inference time by eliminating redundant token representations through layers based on self-attention weights. Several studies have followed <ref type="bibr" coords="2,199.70,156.00,90.34,9.46" target="#b21">(Kim and Cho, 2021;</ref><ref type="bibr" coords="2,70.34,169.55,76.46,9.46" target="#b53">Wang et al., 2021)</ref>; However, they usually optimize a single token elimination configuration across the entire dataset, resulting in a static model. In addition, their token selection strategies are based on attention weights which can result in a suboptimal solution <ref type="bibr" coords="2,108.75,237.30,68.14,9.46" target="#b59">(Ye et al., 2021)</ref>.</p><p>In this work, we introduce Adaptive Length Reduction (AdapLeR). Instead of relying on attention weights, our method trains a set of Contribution Predictors (CP) to estimate tokens' saliency scores at inference. We show that this choice results in more reliable scores than attention weights in measuring tokens' contributions. The most related study to ours is TR-BERT <ref type="bibr" coords="2,216.98,349.61,72.89,9.46" target="#b59">(Ye et al., 2021)</ref> which leverages reinforcement learning to develop an input-adaptive token selection policy network. However, as pointed out by the authors, the problem has a large search space, making it difficult for RL to solve. To mitigate this, they resorted to extra heuristics such as imitation learning <ref type="bibr" coords="2,226.64,430.91,63.86,9.46;2,70.86,444.46,24.94,9.46" target="#b16">(Hussein et al., 2017)</ref> for warming up the training of the policy network, action sampling for limiting the search space, and knowledge distillation for transferring knowledge from the intact backbone fine-tuned model. All of these steps significantly increase the training cost. Hence, they only perform token selection at two layers. In contrast, we propose a simple but effective method to gradually eliminate tokens in each layer throughout the training phase using a soft-removal function which allows the model to be adaptable to various inputs in a batch-wise mode. It is also worth noting in contrast to our approach above studies are based on top-k operations for identifying the k most important tokens during training or inference, which can be expensive without a specific hardware architecture <ref type="bibr" coords="2,233.28,647.71,57.22,9.46;2,70.86,661.26,23.48,9.46" target="#b53">(Wang et al., 2021)</ref>.</p><p>In summary, our contributions are threefold:</p><p>• We couple a simple Contribution Predictor (CP) with each layer of the model to estimate tokens' contribution scores to eliminate redundant representations. • Instead of an instant token removal, we gradually mask out less contributing token repre-sentations by employing a novel soft-removal function. • We also show the superiority of our token selection strategy over the other widely used strategies by using human rationales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" coords="2,306.14,152.62,80.83,10.75">Background</head><p>2.1 Self-attention Weights</p><p>Self-attention is a core component of the Transformers <ref type="bibr" coords="2,345.14,207.13,102.05,9.46" target="#b50">(Vaswani et al., 2017)</ref> which looks for the relation between different positions of a single sequence of token representations (x 1 , ..., x n ) to build contextualized representations. To this end, each input vector x i is multiplied by the corresponding trainable matrices Q, K, and V to respectively produce query (q i ), key (k i ), and value (v i ) vectors. To construct the output representation z i , a series of weights is computed by the dot product of q i with every k j in all time steps. Before applying a softmax function, these values are divided by a scaling factor and then added to an attention mask vector m, which is zero for positions we wish to attend and -∞ (in practice, -10000) for padded tokens <ref type="bibr" coords="2,337.25,396.83,93.34,9.46" target="#b50">(Vaswani et al., 2017)</ref>. Mathematically, for a single attention head, the weight attention from token x i to token x j in the same input sequence can be written as:</p><formula xml:id="formula_0" coords="2,335.72,458.42,188.71,29.35">α i,j = softmax x j ∈X q i k ⊤ j √ d + m i ∈ R<label>(1)</label></formula><p>The time complexity for this is O(n 2 ) given the dot product q i k ⊤ j , where n is the input sequence length. This impedes the usage of self-attention based models in low-resource settings.</p><p>While self-attention is one of the most white-box components in transformer-based models, relying on raw attention weights as an explanation could be misleading given that they are not necessarily responsible for determining the contribution of each token in the final classifier's decision <ref type="bibr" coords="2,464.87,627.49,56.24,9.46;2,306.14,641.04,49.33,9.46" target="#b17">(Jain and Wallace, 2019;</ref><ref type="bibr" coords="2,358.75,641.04,115.24,9.46" target="#b40">Serrano and Smith, 2019;</ref><ref type="bibr" coords="2,477.27,641.04,47.16,9.46;2,306.14,654.59,67.46,9.46" target="#b0">Abnar and Zuidema, 2020)</ref>. This is based on the fact that raw attentions are being faithful to the local mixture of information in each layer and are unable to obtain a global perspective of the information flow through the entire model <ref type="bibr" coords="2,379.77,708.79,89.67,9.46" target="#b34">(Pascual et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" coords="2,306.14,731.85,171.63,9.81">Gradient-based Saliency Scores</head><p>Gradient-based methods provide alternatives to attention weights to compute the importance of a</p><p>[CLS] this stock has risen [SEP]</p><p>. . .</p><p>[CLS] this stock has risen [SEP]</p><p>. . .</p><p>Layer ℓ+1 Inputs: Layer ℓ Outputs: [CLS] this stock has risen [SEP] . . . specific input feature. Despite having been widely utilized in other fields earlier <ref type="bibr" coords="3,198.82,319.51,91.23,9.46" target="#b1">(Ancona et al., 2018;</ref><ref type="bibr" coords="3,70.86,333.06,101.18,9.46" target="#b42">Simonyan et al., 2013;</ref><ref type="bibr" coords="3,175.11,333.06,114.93,9.46" target="#b48">Sundararajan et al., 2017;</ref><ref type="bibr" coords="3,70.86,346.61,91.58,9.46" target="#b43">Smilkov et al., 2017)</ref>, they have only recently become popular in NLP studies <ref type="bibr" coords="3,208.99,360.16,77.94,9.46;3,70.86,373.71,61.55,9.46" target="#b4">(Bastings and Filippova, 2020;</ref><ref type="bibr" coords="3,135.97,373.71,67.56,9.46" target="#b24">Li et al., 2016;</ref><ref type="bibr" coords="3,207.09,373.71,79.16,9.46" target="#b60">Yuan et al., 2019)</ref>. These methods are based on computing the firstorder derivative of the output logit y c w.r.t. the input embedding h 0 i (initial hidden states), where c could be true class label to find the most important input features or the predicted class to interpret model's behavior. After taking the norm of output derivatives, we get sensitivity <ref type="bibr" coords="3,197.79,468.55,88.10,9.46" target="#b1">(Ancona et al., 2018)</ref>, which indicates the changes in model's output with respect to the changes in specific input dimensions. Instead, by multiplying gradients with input features, we arrive at gradient×input <ref type="bibr" coords="3,227.70,522.75,61.44,9.46;3,70.86,536.30,72.38,9.46" target="#b4">(Bastings and Filippova, 2020)</ref>, also known as saliency, which also considers the direction of input vectors to determine the most important tokens. Since these scores are computed for each dimension of embedding vectors, an aggregation method such as L2 norm or mean is needed to produce one score per input token <ref type="bibr" coords="3,122.87,617.60,107.20,9.46">(Atanasova et al., 2020a)</ref>:</p><formula xml:id="formula_1" coords="3,136.27,639.38,152.86,27.33">S i =∥ ∂y c ∂h 0 i ⊙ h 0 i ∥ 2 (2)</formula><p>3 Methodology</p><p>As shown in Figure <ref type="figure" coords="3,157.79,696.19,4.05,9.46" target="#fig_2">1</ref>, our approach relies on dropping low contributing tokens in each layer and passing only the more important ones to the next. Therefore, one important step is to measure the importance of each token. To this end, we opted for saliency scores which have been recently shown as a reliable criterion in measuring token's contributions <ref type="bibr" coords="3,351.30,319.51,136.58,9.46" target="#b4">(Bastings and Filippova, 2020;</ref><ref type="bibr" coords="3,490.59,319.51,33.84,9.46;3,306.14,333.06,51.69,9.46" target="#b34">Pascual et al., 2021)</ref>. In Section 5.1 we will show results for a series quantitative analyses that supports this choice. In what follows, we first describe how we estimate saliency scores at inference time using a set of Contribution Predictors (CPs) and then elaborate on how we leverage these predictors during inference (Section 3.2) and training (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" coords="3,306.14,444.56,132.31,9.81">Contribution Predictor</head><p>Computing gradients during inference is problematic as backpropagation computation prolongs inference time, which is contrary to our main goal.</p><p>To circumvent this, we simply add a CP after each layer ℓ in the model to estimate contribution score for each token representation, i.e., Sℓ i . The model then decides on the tokens that should be passed to the next layer based on the values of Sℓ i . CP computes Sℓ i for each token using an MLP followed by a softmax activation function. We argue that, despite being limited in learning capacity, the MLP is sufficient for estimating scores that are more generalized and relevant than vanilla saliency values. We will present a quantitative analysis on this topic in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" coords="3,306.14,687.11,101.00,9.81">Model Inference</head><p>Most BERT-based models consist of L encoder layers. The input sequence of n tokens is usually passed through an embedding layer to build the initial hidden states of the model h 0 . Each encoder layer then produces the next hidden states using the ones from the previous layer:</p><formula xml:id="formula_2" coords="4,132.67,92.03,156.46,13.27">h ℓ = Encoder ℓ (h ℓ-1 )<label>(3)</label></formula><p>In our approach, we eliminate less contributing token representations before delivering hidden states to the next encoder. Tokens are selected based on the contribution scores Sℓ obtained from the CP of the corresponding layer ℓ. As the sum of these scores is equal to one, a uniform level indicates that all tokens contribute equally to the prediction and should be retained. On the other hand, the lower-scoring tokens could be viewed as unnecessary tokens if the contribution scores are concentrated only on a subset of tokens. Given that the final classification head uses the last hidden state of the [CLS] token, we preserve this token's representation in all layers. Despite preserving this, other tokens might be removed from a layer when [CLS] has a significantly high estimated contribution score than others. Based on this intuition, we define a cutoff threshold based on the uniform level as: δ ℓ = η ℓ • 1 /n with 0 &lt; η ℓ ≤ 1 to distinguish important tokens. Tokens are considered important if their contribution score exceeds δ (which is a value equal or smaller than the uniform score). Intuitively, a larger η provides a higher δ cutoff level, thereby dropping a larger number of tokens, hence, yielding more speedup. The value of η determines the extent to which we can rely on CP's estimations. In case the estimations of CP are deemed to be inaccurate, its impact can be reduced by lowering η. We train each layer's η ℓ using an auxiliary training objective, which allows the model to adjust the cutoff value to control the speedup-performance tradeoff. Also, since each input instance has a different computational path during token removal process, it is obvious that at inference time, the batch size should be equal to one (single instance usage), similarly to other dynamic approaches <ref type="bibr" coords="4,163.89,602.84,81.69,9.46" target="#b64">(Zhou et al., 2020;</ref><ref type="bibr" coords="4,248.30,602.84,42.20,9.46;4,70.86,616.39,24.98,9.46" target="#b27">Liu et al., 2020;</ref><ref type="bibr" coords="4,98.55,616.39,66.15,9.46" target="#b59">Ye et al., 2021;</ref><ref type="bibr" coords="4,167.42,616.39,102.55,9.46" target="#b10">Eyzaguirre et al., 2021;</ref><ref type="bibr" coords="4,272.69,616.39,16.45,9.46;4,70.86,629.94,50.15,9.46" target="#b56">Xin et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" coords="4,70.86,651.18,97.39,9.81">Model Training</head><p>Training consists of three phases: initial finetuning, saliency extraction, and adaptive length retraining. In the first phase, we simply fine-tune the backbone model (BERT) on a given target task. We then extract the saliencies of three top-perfroming checkpoints from the fine-tuning process and compute the average of them to mitigate potential inconsistencies in saliency scores (cf. Section 2.2). The final step is to train a pre-trained model using an adaptive length reduction procedure. In this phase, a non-linear function gradually fades out the representations throughout the training process. Each CP is jointly trained with the rest of the model using the saliencies extracted in the previous phase alongside with the target task labels. We also define a speedup tuning objective to determine the thresholds (via tuning η) to control the performance-speedup trade-off. In the following, we elaborate on the procedure.</p><p>Soft-removal function. During training, if tokens are immediately dropped similarly to the inference mode, the effect of dropping tokens cannot be captured using a gradient backpropagation procedure. Using batch-wise training in this scenario will also be problematic as the structure will vary with each example. Hence, inspired by the padding mechanism of self-attention models <ref type="bibr" coords="4,305.78,554.08,95.33,9.46" target="#b50">(Vaswani et al., 2017)</ref> we introduce a new procedure that gradually masks out less contributing token representations. In each layer, after predicting contribution scores, instead of instantly removing the token representations, we accumulate a negative mask to the attention mask vector M using a soft-removal function:</p><formula xml:id="formula_3" coords="4,313.29,655.88,211.13,53.33">m - i ( Sℓ i ) =        λ adj ( Sℓ i -δ ℓ ) - β λ Sℓ i &lt; δ ℓ ( Sℓ i -1)β (1 -δ ℓ )λ Sℓ i ≥ δ ℓ (4)</formula><p>This function consists of two main zones (Figure <ref type="figure" coords="4,306.14,736.84,3.92,9.46" target="#fig_3">2</ref>). In the first term, the less important tokens with scores lower than the threshold (δ ℓ ) are assigned higher negative masking as they get more distant from δ. The slope is determined by λ adj = λ /δ, where λ is a hyperparameter that is increased exponentially after each epoch (e.g., λ ← 10 × λ after finishing each epoch). Increasing λ makes the soft-removal function stronger and more decisive in masking the representations. To avoid undergoing zero gradients during training, we define 0 &lt; β &lt; 0.1 to construct a small negative slope (similar to the well known Leaky-ReLU of <ref type="bibr" coords="5,265.03,183.10,24.10,9.46;5,70.86,196.65,44.32,9.46" target="#b30">Maas et al. 2013</ref>) for those tokens with higher contributing scores than δ ℓ threshold. Consider a scenario in which η ℓ sharply drops, causing most of Sℓ i get over the δ ℓ threshold. In this case, the non-zero value in the second term of Equation <ref type="formula" coords="5,207.04,250.85,4.06,9.46">4</ref>, which facilitates optimizing η ℓ .</p><p>Training the Contribution Predictors. The CPs are trained by an additional term which is based on the KL-divergence 1 of each layer's CP output with the extracted saliencies. The main training objective is a minimization of the following loss:</p><formula xml:id="formula_4" coords="5,140.38,363.26,148.75,10.81">L = L CE + γL CP (5)</formula><p>Where γ is a hyperparameter which that specifies the amount of emphasis on the CP training loss:</p><formula xml:id="formula_5" coords="5,103.89,423.46,185.25,72.59">L CP = L-1 ℓ=0 (L -ℓ)D KL ( Ŝℓ || Sℓ ) = L-1 ℓ=0 (L -ℓ) N i=1 Ŝℓ i log( Ŝℓ i Sℓ i ) (6)</formula><p>Since S is based on the input embeddings, the [CLS] token usually shows a low amount of contribution due to not having any contextualism in the input. As we leverage the representation of the [CLS] token in the last layer for classification, this token acts as a pooler and gathers information about the context of the input. In other words, the token can potentially have more contribution as it passes through the model. To this end, we amplify the contribution score of [CLS] and renormalize the distribution ( Ŝℓ ) with a trainable parameter θ ℓ :</p><formula xml:id="formula_6" coords="5,108.93,664.19,180.21,29.68">Ŝℓ i = θ ℓ S ℓ 1 1[i = 1] + S ℓ i 1[i &gt; 1] θ ℓ S ℓ 1 + n i=2 S ℓ i (7)</formula><p>By this procedure, the next objective (discussed in the next paragraph) will have the capability of tuning the amount of pooling, consequently controlling the amount of speedup. Larger θ push the 1 Inclusive KL loss. Check Appendix A.</p><p>CPs to shift the contribution towards the [CLS] token to gather most of the task-specific information and avoids carrying redundant tokens through the model.</p><p>Speedup Tuning. In the speedup tuning process, we combine the cross-entropy loss of the target classification task with a length loss which is the expected number of unmasked token representations in all layers. Considering that we have a non-positive and continuous attention mask M , the length loss of a single layer would be the summation over the exponential of the mask values exp(m i ) to map the masking range [-∞, 0] to a [0 (fully masked/removed), 1 (fully retained)] bound.</p><formula xml:id="formula_7" coords="5,347.00,294.71,177.42,50.58">L SPD./PERF. = L CE + ϕL LENGTH L LENGTH = L l=1 n i=1 exp(m ℓ i )<label>(8)</label></formula><p>Equation 8 demonstrates how the length loss is computed inside the model and how it is added to the main classification loss. During training, we assign a separate optimization process which tunes η and θ to adjust the thresholds and the amount of [CLS] pooling<ref type="foot" coords="5,372.21,423.60,3.99,6.91" target="#foot_0">foot_0</ref> alongside with the CP training. The reason that this objective is treated as a separate problem instead of merging it with the previous one, is because in the latter case the CPs could be influenced by the length loss and try to manipulate the contribution scores for some tokens regardless of their real influence. So in other words, the first objective is to solve the task and make it explainable with the CPs, and the secondary objective builds the speedup using tuning the threshold levels and the amount of pooling in each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" coords="5,306.14,584.97,83.02,10.75;5,306.14,607.27,63.93,9.81">Experiments 4.1 Datasets</head><p>To verify the effectiveness of AdapLeR on inference speedup, we selected eight various text classification datasets. In order to incorporate a variety of tasks, we utilized SST-2 <ref type="bibr" coords="5,421.14,666.36,85.63,9.46" target="#b44">(Socher et al., 2013)</ref> and IMDB <ref type="bibr" coords="5,338.58,679.91,85.56,9.46" target="#b29">(Maas et al., 2011)</ref> for sentiment, MRPC <ref type="bibr" coords="5,305.78,693.46,122.75,9.46" target="#b8">(Dolan and Brockett, 2005)</ref> for paraphrase, AG's News <ref type="bibr" coords="5,333.89,707.01,87.77,9.46" target="#b62">(Zhang et al., 2015)</ref> for topic classification, DBpedia <ref type="bibr" coords="5,349.68,720.56,105.26,9.46" target="#b23">(Lehmann et al., 2015)</ref> for knowledge extraction, MNLI <ref type="bibr" coords="5,386.79,734.11,99.41,9.46" target="#b54">(Williams et al., 2018)</ref> for NLI, Model SST-2 IMDB HateXplain MRPC MNLI QNLI AG's news DBpedia Acc. Speedup Acc. Speedup Acc Speedup F1. Speedup Acc. Speedup Acc. Speedup Acc. Speedup Acc. Speedup BERT 92.7 1.00x 93.8 1.00x 68.3 1.00x 87.5 1.00x 84.2 1.00x 90.3 1.00x 94.4 1.00x 99.3 1.00x DistilBERT 92.2 2.00x 92.9 2.00x 68.2 2.00x 88.0 2.00x 81.8 2.00x 88.1 2.00x 94.2 2.00x 99.3 2.00x PoWER-BERT 92.1 1.18x 92.2 1.70x 66.9 2.69x 88.0 1.07x 82.9 1.10x 89.7 1.23x 92.1 12.50x 98.1 14.80x TR-BERT 92.1 1.46x 93.2 2.90x 67.9 2.23x 81.9 1.16x 84.8 1.00x 89.0 1.09x 93.2 10.20x 98.9 10.01x AdapLeR 92.3 1.49x 91.7 3.21x 68.6 4.73x 87.6 1.27x 82.9 1.42x 89.3 1.47x 92.5 17.10x 98.9 22.23x</p><p>Table <ref type="table" coords="6,94.79,170.45,3.85,8.64">1</ref>: Comparison of our proposed method (AdapLeR) with other baselines in eight classification tasks in terms of performance and speedup. For each dataset the corresponding metric has been reported (Accuracy: Acc., F1: F-1 Score). In the MNLI task, the speedup and performance values are the average of the evaluations on the matched and mismatched test sets.</p><p>QNLI <ref type="bibr" coords="6,99.26,240.18,95.72,9.46" target="#b35">(Rajpurkar et al., 2016</ref>) for question answering, and HateXplain <ref type="bibr" coords="6,160.32,253.73,93.95,9.46" target="#b31">(Mathew et al., 2021)</ref> for hate speech. <ref type="foot" coords="6,103.66,265.23,3.99,6.91" target="#foot_1">3</ref> Evaluations are based on the test split of each dataset. For those datasets that are in the GLUE Benchmark <ref type="bibr" coords="6,154.90,294.38,80.93,9.46" target="#b52">(Wang et al., 2018)</ref>, test results were acquired by submitting the test predictions to the evaluation server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" coords="6,70.86,345.91,117.58,9.81">Experimental Setup</head><p>As our baseline, we report results for the pretrained BERT model (base-uncased) <ref type="bibr" coords="6,231.10,378.78,59.39,9.46;6,70.86,392.33,25.96,9.46" target="#b6">(Devlin et al., 2019)</ref> which is also the backbone of AdapLeR. We also compare against three other approaches: DistilBERT (uncased) <ref type="bibr" coords="6,173.12,419.43,90.50,9.46" target="#b36">(Sanh et al., 2019)</ref> as a static compression method, PoWER-BERT and TR-BERT as two strong length reduction methods (cf. Sec. 1). We used the provided implementations and suggested hyperparameters<ref type="foot" coords="6,247.19,471.58,3.99,6.91" target="#foot_2">foot_2</ref> to train these baselines. To fine-tune the backbone model, we used same hyperparameters over all tasks (see Section D for details). The backbone model and our model implementation is based on the Hug-gingFace's Transformers library <ref type="bibr" coords="6,210.36,541.38,76.09,9.46" target="#b55">(Wolf et al., 2020)</ref>.</p><p>Trainings and evaluations were conducted on a dual 2080Ti 11GB GPU machine with multiple runs.</p><p>Hyperparameter Selection. Overall, we introduced four hyperparameters (γ, ϕ, λ, β)<ref type="foot" coords="6,240.45,603.37,3.99,6.91" target="#foot_3">foot_3</ref> which are involved in the training process. Among these, ϕ and γ are the primary terms that have considerable effects on AdapLeR's downstream performance and speedup. This makes our approach comparable to existing techniques <ref type="bibr" coords="6,166.99,673.16,83.08,9.46" target="#b12">(Goyal et al., 2020;</ref><ref type="bibr" coords="6,252.80,673.16,37.70,9.46;6,70.86,686.71,25.96,9.46" target="#b59">Ye et al., 2021)</ref> which usually have two or three hyperparameters adjusted per task. We used grid search to find the optimal values for these two terms, while keeping the other hyperparameters constant over all datasets. Hyperparamter selection is further discussed in Section D.</p><p>FLOPs Computation. We followed <ref type="bibr" coords="6,485.67,302.54,40.66,9.46;6,305.78,316.09,29.67,9.46" target="#b59">Ye et al. (2021)</ref> and <ref type="bibr" coords="6,358.81,316.09,74.91,9.46" target="#b27">Liu et al. (2020)</ref> and measured computational complexity in terms of FLOPs, i.e., the number of floating-point operations (FLOPs) in a single inference procedure. This allows us to assess models' speedups independently of their operating environment (e.g., CPU/GPU). The total FLOPs of a given model is a summation of the measured FLOPs over all test examples. Then, a model's speedup can be defined as the total FLOPs measured on BERT (our baseline) divided by the corresponding model's total FLOPs. To have a fair comparison, we also computed FLOPs for PoWER-BERT in a single instance mode, described in Section C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" coords="6,306.14,515.00,58.49,9.81">Results</head><p>Table <ref type="table" coords="6,336.85,533.36,5.56,9.46">1</ref> shows performance and speedup for AdapLeR and other comparison models across eight different datasets. While preserving the same level of performance, AdapLeR outperforms other techniques in terms of speedup across all tasks (ranging from +0.2x to +7.4x compared to the best model in each dataset).</p><p>It is noteworthy that the results also reveal some form of dependency on the type of the tasks. Some tasks may need less amount of contextualism during inference and could be classified by using only a fraction of input tokens. For instance, in AG's News, the topic of a sentence might be identifiable with a single token (e.g., soccer → Topic: Sports, see Figure <ref type="figure" coords="6,358.83,723.29,5.56,9.46">6</ref> in the Appendix for an example). PoWER-BERT adopts attention weights in its token selection which requires at least one layer of computation to be determined, and TR-BERT ap-</p><p>1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 Speedup 0.86 0.87 0.88 0.89 0.90 0.91 0.92 0.93 Accuracy SST-2 TR PoWER AdapLeR 2 3 4 5 6 7 Speedup 0.64 0.65 0.66 0.67 0.68 0.69 0.70 Accuracy HateXplain TR PoWER AdapLeR plies token elimination only in two layers to reduce the training search space. In contrast, our procedure performs token elimination for all layers of the model, enabling a more effective removal of redundant tokens. On the other hand, we observe that TR-BERT and PoWER-BERT lack any speedup gains for tasks such as QNLI, MNLI, and MRPC which need a higher degree of contextualism during inference. However, AdapLeR can offer some speedups even for these tasks.</p><p>Speedup-Performance Tradeoff. To provide a closer look at the efficiency of AdapLeR in comparison with the other state-of-the-art length reduction methods, we illustrate speedup-accuracy curves in Figure <ref type="figure" coords="7,148.01,587.79,4.17,9.46" target="#fig_4">3</ref>. We provide these curves for two tasks in which other length reduction methods show comparable speedups to AdapLeR. For each curve, the points were obtained by tuning the most influential hyperparameters of the corresponding model. As we can see, AdapLeR significantly outperforms the other two approaches in all two tasks. An interesting observation here is that the curves for TR-BERT and AdapLeR are much higher than that of PoWER-BERT. This can be attributed to the input-adaptive procedure employed by the former two methods for determining the number of reduced tokens (whereas PoWER-BERT adopts a fixed retention configuration in token elimination).</p><p>Movie Reviews MultiRC Strategy Acc. Speedup Acc. Speedup Full input 93.3 1.0x 67.7 1.0x Human rationale 96.7 3.7x 76.6 4.6x Saliency 92.3 3.7x 66.4 4.4x Attention ALL 78.5 3.7x 62.9 4.4x Attention [CLS] 70.3 3.7x 63.7 4.4x Table 2: Accuracy and speedup when the most important input tokens during fine-tuning are computed based on attention and saliency strategies and human rationale (the upper bound). The bold values indicate the best corresponding strategy for each task (the closest performance to the upper bound).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" coords="7,306.14,278.53,61.12,10.75">Analysis</head><p>In this section, we first conduct an experiment to support our choice of saliency scores as a supervision in measuring the importance of token representations. Next, we evaluate the behavior of Contribution Predictors in identifying the most important tokens in the AdapLeR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" coords="7,306.14,392.59,162.15,9.81">Rationale as an Upper Bound</head><p>A natural question that arises when dealing with token pruning is that of importance measure: what is the most appropriate criterion for assessing the relative importance of tokens within a sentence?</p><p>We resort to human rationale as a reliable upper bound for measuring token importance. To this end, we used the ERASER benchmark <ref type="bibr" coords="7,499.06,492.55,21.74,9.46;7,306.14,506.10,77.23,9.46" target="#b7">(DeYoung et al., 2020)</ref>, which contains multiple tasks for which important spans of the input text have been highlighted as supporting evidence (aka "rationale") by human. Among the tasks in the benchmark, we opted for two diverse classification tasks: Movie reviews <ref type="bibr" coords="7,434.13,573.85,91.66,9.46;7,306.14,587.40,25.96,9.46" target="#b61">(Zaidan and Eisner, 2008)</ref> and MultiRC <ref type="bibr" coords="7,399.79,587.40,103.35,9.46" target="#b20">(Khashabi et al., 2018)</ref>. In the former task, the model predicts the sentiment of the passage. Whereas the latter contains a passage, a question, and multiple candidate answers, which is cast as a binary classification task of passage/question/answer triplets in the ERASER benchmark.</p><p>In order to verify the reliability of human rationales, we fine-tuned BERT based on the rationales only, i.e., by excluding those tokens that are not highlighted as being important in the input. In Table 2, the first two rows show the performance of BERT on the two tasks with full input and with human rationales only. We see that fine-tuning merely on rationales not only yields less computation cost, but also results in a better performance when compared with the full input setting. Obviously, human annotations are not available for a whole range of downstream NLP tasks; therefore, this criterion is infeasible in practice and can only be viewed as an upper bound for evaluating different strategies in measuring token importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" coords="8,70.86,388.42,126.12,9.81">Saliency vs. Attention</head><p>We investigated the effectiveness of saliency and self-attention weights as two commonly used strategies for measuring the importance of tokens in pre-trained language models. To compute these, we first fine-tuned BERT with all tokens in the input for a given target task. We then obtained saliency scores with respect to the tokens in the input embedding layer. This brings about two advantages. Firstly, representations in the embedding layer are non-contextualized, allowing us to measure the importance of each token independently from the others. Secondly, the backpropagation of gradients through layers to the beginning of the model provides us with aggregated values for the relative importance of each token based on the entire model. Similarly, we aggregated the selfattention weights across all layers of the model using a post-processed variant of attentions called attention rollout <ref type="bibr" coords="8,141.91,653.76,115.74,9.46" target="#b0">(Abnar and Zuidema, 2020)</ref>, a popular technique in which the attention weight matrix in each layer is multiplied with the preceding ones to form aggregated attention values.</p><p>To assign an importance score to each token, we examined two different interpretation of attention weights. The first strategy is the one adopted by PoWER-BERT <ref type="bibr" coords="8,142.15,750.39,88.37,9.46" target="#b12">(Goyal et al., 2020)</ref> in which for each token we accumulate attention values from other tokens. Additionally, we measured how much the [CLS] token attends to each token in the sentence, a strategy which has been widely used in interpretability studies around BERT <ref type="bibr" coords="8,473.85,306.10,50.58,9.46;8,306.14,319.65,69.31,9.46" target="#b0">(Abnar and Zuidema, 2020;</ref><ref type="bibr" coords="8,378.20,319.65,147.13,9.46" target="#b5">Chrysostomou and Aletras, 2021;</ref><ref type="bibr" coords="8,305.94,333.00,116.43,9.65">Jain et al., 2020, inter alia)</ref>. For a fair comparison, for each sentence in the test set, we selected the top-k salient and attended words, with k being the number of words that are annotated as rationales.</p><p>Results in Table <ref type="table" coords="8,391.37,388.94,5.56,9.46">2</ref> show that fine-tuning on the most salient tokens outperforms that based on the most attended tokens. This denotes that saliency is a better indicator for the importance of tokens. Nonetheless, recent length reduction techniques <ref type="bibr" coords="8,305.78,456.69,90.01,9.46" target="#b12">(Goyal et al., 2020;</ref><ref type="bibr" coords="8,399.87,456.69,94.78,9.46" target="#b21">Kim and Cho, 2021;</ref><ref type="bibr" coords="8,498.72,456.69,25.71,9.46;8,306.14,470.24,51.19,9.46" target="#b53">Wang et al., 2021)</ref> have mostly adopted attention weights as their criterion for selecting important tokens. Computing these weights is convenient as they are already computed during the forward pass, whereas computing saliency requires an additional backpropagation step. Note that in our approach, saliency scores are easily estimated within inference time by the pre-trained CPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" coords="8,306.14,592.38,185.86,9.81">Contribution Predictor Evaluation</head><p>In this section we validate our Contribution Predictors in selecting the most contributed tokens. Figure <ref type="figure" coords="8,322.81,640.45,5.46,9.46">4</ref> illustrates two examples from the SST-2 and QNLI datasets in which CPs identify and gradually drop the irrelevant tokens through layers, finally focusing mostly on the most important token representations; pedestrian (adjective) in SST-2 and tesla coil in the passage part of QNLI (both of which are highly aligned with human rationale).</p><p>In order to quantify the extent to which AdapLeR's CPs can preserve rationales without requiring direct human annotations in an unsuper- vised manner we carried out the following experiment. To investigate the effectiveness of trained CPs in predicting human rationales we computed the output scores of CPs in AdapLeR for each token representation in each layer. We also fine-tuned a BERT model on the Movie Review dataset and computed layer-wise raw attention, attention rollout, and saliency scores for each token representation. Since human rationales are annotated at the word level, we sum the scores across tokens corresponding to each word to arrive at word-level importance scores. In addition to these soft scores, we used the uniform-level threshold (i.e., 1 /n) to reach a binary score indicating tokens selected in each layer.</p><p>As for evaluation, we used the Average Precision (AP) and False Positive Rate (FPR) metrics by comparing the remaining tokens to the human rationale annotations. The first metric measures whether the model assigns higher continuous scores to those tokens that are annotated by humans as rationales. Whereas, the intuition behind the second metric is how many irrelevant tokens are selected by the model to be passed to subsequent layers. We used soft scores for computing AP and binary scores for computing FPR.</p><p>Figure <ref type="figure" coords="9,113.80,696.19,5.56,9.46" target="#fig_6">5</ref> shows the agreement between human rationales and the selected tokens based on the two metrics. In comparison with the other widely used strategies for selecting important tokens, such as salinecy and attention, our CPs have significantly less false positive rate in preserving ratio-nales through layers. Despite having similar FPRs at the final layer, CP is preferable to attention in that it can better identify rationales at the earlier layers, allowing the model to combine the most relevant token representations when building the final one. This in turn results in better performance, as was also shown in the previous experiment in Section 5.2. Also, we see that the curve of mAP for saliency is consistently higher than other strategies in terms of alignment with human rationales which supports our choice of saliency as a measure for token importance.</p><p>Finally, we note that there is a line of research that attempts at guiding models to perform humanlike reasoning by training rationale generation simultaneously with the target task that requires human annotation <ref type="bibr" coords="9,381.11,291.79,116.03,9.46">(Atanasova et al., 2020b;</ref><ref type="bibr" coords="9,501.56,291.79,22.87,9.46;9,306.14,305.34,50.41,9.46" target="#b63">Zhao et al., 2020;</ref><ref type="bibr" coords="9,359.20,305.34,61.21,9.46" target="#b25">Li et al., 2018)</ref>. As a by-product of the contribution estimation process, our trained CPs are able to generate these rationales at inference without the need for human-generated annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" coords="9,306.14,369.59,75.07,10.75">Conclusion</head><p>In this paper, we introduced AdapLeR, a novel method that accelerates inference by dynamically identifying and dropping less contributing token representations through layers of BERT-based models. Specifically, AdapLeR accomplishes this by training a set of Contribution Predictors based on saliencies extracted from a fine-tuned model and applying a gradual masking technique to simulate input-adaptive token removal during training. Empirical results on eight diverse text classification tasks show considerable improvements over existing methods. Furthermore, we demonstrated that contribution predictors generate rationales that are highly in line with those manually specified by humans. As future work, we aim to apply our technique to more tasks and see whether it can be adapted to those tasks that require all token representations to be present in the final layer of the model (e.g., question answering). Additionally, combining our width-based strategy with a depthbased one (e.g., early exiting) might potentially yield greater efficiency, something we plan to pursue as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="9,306.14,714.07,81.80,10.75">Broader Impact</head><p>Using our proposed method, pre-trained language models can use fewer FLOPs, reducing energy use and carbon emissions <ref type="bibr" coords="9,402.80,763.94,101.82,9.46">(Schwartz et al., 2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,70.86,605.83,188.09,10.75">A Inclusive KL Loss Consideration</head><p>We opted for an inclusive KL loss since CPs should be trained to cover all tokens considered important by saliency and not to be mode seeking (i.e., covering a subset of high contributing tokens considered by the saliency scores.). Suppose an exclusive KL is selected. Due to the limited learning capacity of the CP and miscalculation possibility from the saliency, the CP may be trained to maximize its contribution on noninformative tokens. While in an inclusive setting, it trains to extend its coverage over all high-saliency tokens.</p><p>Additionally, our initial research indicated that using a symmetric loss (e.g. Jensen-Shannon divergence) would produce similar results but with a significantly longer convergence time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="13,306.14,137.80,109.07,10.75">B Optimization of θ</head><p>In Section 3.3, we introduced θ ℓ as a trainable parameter that increases the saliency score of <ref type="bibr" coords="13,494.24,173.24,26.75,9.46">[CLS]</ref>. We can deduce from Equations 6 and 7 that this parameter does not exist in the model's computational DAG and we need to compute the derivative of Sℓ w.r.t. θ ℓ to train this parameter. Hence, first we assume that Sℓ is a close estimate of Ŝℓ (due to the CPs' training objective). Second, using a dummy variable θ ℓ d -that is involved in the computational graph and is always equal to 1-we reformulate Sℓ :</p><formula xml:id="formula_8" coords="13,324.65,313.15,199.77,31.70">Ŝℓ i ≈ Sℓ i = θ ℓ d Sℓ 1 1[i = 1] + Sℓ i 1[i &gt; 1] θ ℓ d Sℓ 1 + n i=2 Sℓ i<label>(9)</label></formula><p>This reformulation is valid due to θ ℓ d = 1 and n i=1</p><p>Sℓ i = 1. Now we compute the partial derivative w.r.t. θ ℓ d which is the gradient that is computed in the backpropagation:</p><formula xml:id="formula_9" coords="13,320.22,417.20,190.57,41.07">∂ Sℓ i ∂θ ℓ d = Sℓ 1 ( n i=2 Sℓ i 1[i = 1] -Sℓ i 1[i &gt; 1]) (θ ℓ d Sℓ 1 + n i=2 Sℓ i ) 2<label>(</label></formula><p>10) By knowing that θ ℓ d = 1:</p><formula xml:id="formula_10" coords="13,312.79,483.53,211.63,30.75">∂ Sℓ i ∂θ ℓ d = Sℓ 1 ((1 -Sℓ 1 )1[i = 1] -Sℓ i 1[i &gt; 1])<label>(11)</label></formula><p>Now using our initial assumption ( Ŝℓ i ≈ Sℓ i ), we can substitute Sℓ i with Ŝℓ i based on Equation <ref type="formula" coords="13,502.03,540.43,4.24,9.46">7</ref>:</p><formula xml:id="formula_11" coords="13,315.53,561.33,199.51,72.26">∂ Sℓ i ∂θ ℓ d = Ŝℓ 1 ((1 -Ŝℓ 1 )1[i = 1] -Ŝℓ i 1[i &gt; 1]) = θ ℓ S ℓ 1 ( n i=2 S ℓ i 1[i = 1] -S ℓ i 1[i &gt; 1]) (θ ℓ S ℓ 1 + n i=2 S ℓ i ) 2<label>(</label></formula><p>12) In addition, the gradient of Ŝℓ i w.r.t. θ ℓ is as follows (cf. Equation <ref type="formula" coords="13,366.13,651.24,4.04,9.46">7</ref>): </p><formula xml:id="formula_12" coords="13,320.22,671.37,190.57,40.05">∂ Ŝℓ i ∂θ ℓ = S ℓ 1 ( n i=2 S ℓ i 1[i = 1] -S ℓ i 1[i &gt; 1]) (θ ℓ S ℓ 1 + n i=2 S ℓ i ) 2<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="14,70.86,125.30,204.11,10.75">C Evaluating PoWER-BERT in Single</head><p>Instance Mode</p><p>Due to the static structure of PoWER-BERT, the speedup ratios reported in <ref type="bibr" coords="14,188.87,175.49,83.91,9.46" target="#b12">Goyal et al. (2020)</ref> are based on wall time acceleration with batch-wise inference procedure. This means that some inputs might need extra padding to make all inputs with the same token length. However, since our approach and other dynamic approaches are based on single instance inference, in our procedure inputs are fed without being padded. To even out this discrepancy, we apply a single instance flops computation on the PoWER-BERT, which means we compute the computational cost for all input lengths that appear in the test dataset. Some instnaces may have shorter input length than some values in the resulting retention configuration (number of tokens that are retained in each layer). To overcome this issue, we update the retention configuration by selecting the minimum between the input length and each layers' number of tokens retained, to build a new retention configuration for each input length. For instance, if the retention configuration trained model on a given task be <ref type="bibr" coords="14,267.32,446.49,23.18,9.46">(153,</ref><ref type="bibr" coords="14,70.04,460.04,18.98,9.46">125,</ref><ref type="bibr" coords="14,91.74,460.04,18.98,9.46">111,</ref><ref type="bibr" coords="14,113.44,460.04,18.98,9.46">105,</ref><ref type="bibr" coords="14,135.14,460.04,13.56,9.46">85,</ref><ref type="bibr" coords="14,151.41,460.04,13.56,9.46">80,</ref><ref type="bibr" coords="14,167.69,460.04,13.56,9.46">72,</ref><ref type="bibr" coords="14,183.97,460.04,13.56,9.46">48,</ref><ref type="bibr" coords="14,200.24,460.04,13.56,9.46">35,</ref><ref type="bibr" coords="14,216.51,460.04,13.56,9.46">27,</ref><ref type="bibr" coords="14,232.79,460.04,13.56,9.46">22,</ref><ref type="bibr" coords="14,249.07,460.04,7.83,9.46">5)</ref>, for an input with 75 tokens length, the new configuration which is used for speedup computation will be: <ref type="bibr" coords="14,273.57,487.14,16.92,9.46">(75,</ref><ref type="bibr" coords="14,70.58,500.69,13.64,9.46">75,</ref><ref type="bibr" coords="14,86.95,500.69,13.64,9.46">75,</ref><ref type="bibr" coords="14,103.31,500.69,13.64,9.46">75,</ref><ref type="bibr" coords="14,119.68,500.69,13.64,9.46">75,</ref><ref type="bibr" coords="14,136.04,500.69,13.64,9.46">75,</ref><ref type="bibr" coords="14,152.41,500.69,13.64,9.46">72,</ref><ref type="bibr" coords="14,168.77,500.69,13.64,9.46">48,</ref><ref type="bibr" coords="14,185.13,500.69,13.64,9.46">35,</ref><ref type="bibr" coords="14,201.50,500.69,13.64,9.46">27,</ref><ref type="bibr" coords="14,217.86,500.69,13.64,9.46">22,</ref><ref type="bibr" coords="14,234.23,500.69,7.88,9.46">5)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head coords="14,70.86,524.19,211.53,10.75">D AdapLeR Training Hyperparameters</head><p>For the initial step of fine-tuning BERT, we used the hyperparameters in Table <ref type="table" coords="14,186.93,560.43,4.17,9.46" target="#tab_5">3</ref>. For both fine-tuning and training with length reduction, we employed an AdamW optimizer <ref type="bibr" coords="14,155.71,587.53,134.15,9.46" target="#b28">(Loshchilov and Hutter, 2019)</ref> with a weight decay rate of 0.1, warmup proportion 6% of total training steps and a linear learning rate decay which reaches to zero at the end of training.</p><p>For the adaptive length reduction training step, we also used the same hyperparameters in Table <ref type="table" coords="14,70.86,669.09,5.56,9.46" target="#tab_5">3</ref> with two differences: Since MRPC and CoLA have small training sets, to prolong the gradual softremoval process, we increased the training duration to 10 epochs. Moreover, we increase the learning rate to 3e-5. Other hyperparameters are stated in Table <ref type="table" coords="14,97.39,736.84,4.12,9.46">4</ref>. To set a trend for λ, it needs to start from a small but effective value (10 &lt; λ &lt; 100) and grow exponentially per each epoch to reach an ex-</p><p>Dataset Epoch LR MaxLen. BSZ SST-2 5 2e-5 64 32 IMDB 5 2e-5 512 16 HateXplain 5 3e-5 72 32 MRPC 5 2e-5 128 32 MNLI 3 2e-5 128 32 QNLI 5 2e-5 128 32 AG's News 5 2e-5 128 32 DBpedia 3 2e-5 128 32 Table 4: AdapLeR hyperparameters in each dataset; Since λ increases exponentially on each epoch the coorresponding formula is written. E Statistics of Datasets F Additional Qualitative Examples</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,70.86,236.23,453.56,8.64;3,70.86,248.19,248.61,8.64;3,319.84,246.30,3.34,6.12;3,326.44,246.22,17.85,10.38;3,343.68,247.87,180.75,8.96;3,70.86,260.14,453.73,8.64;3,70.50,272.10,311.46,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: To reduce the inference computation, in each layer (1) the attribution score of the token representation is estimated and (2) based on a reduced uniform-level threshold (δ ℓ = η ℓ/n) token representations with low importance score are removed. Since the final layer's classifier is connected to the [CLS] token and it could act as a pooler within each layer it is the only token that would remain regardless of its score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,306.14,217.33,218.28,8.64;4,306.14,228.96,96.12,8.96;4,402.64,227.39,3.34,6.12;4,410.09,228.96,114.33,8.96;4,306.14,241.24,219.52,8.64;4,305.79,253.19,213.49,8.64"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The soft-removal function plotted with λ ∈ {3, 9, 27, 81} and δ ℓ = 0.25. As λ increases, the removal region (1) gets steeper while the other zone (2), which is almost horizontal, approaches the zero level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,70.86,319.43,218.45,8.64;7,70.50,331.39,220.29,8.64;7,70.86,343.34,218.28,8.64;7,70.86,355.30,58.96,8.64"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracy-Speedup trade-off curve for AdapLeR and two other state-of-the-art reduction methods; TR: TR-BERT, PoWER: PoWER-BERT on two different tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,70.86,195.72,453.57,8.64;8,70.86,207.68,453.91,8.64;8,70.86,219.63,453.56,8.64;8,70.86,231.59,61.15,8.64"><head>SST- 2</head><label>2</label><figDesc>Figure 4: The illustration of contribution scores obtained by CPs in three different layers of the model for two input examples from SST-2 (sentiment) and QNLI (Question-answering NLI) tasks. The contribution scores are shown by color intensity. Only the highlighted token representations are processed in each layer. See more full-layer plots in the appendix 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,70.86,267.93,218.28,8.64;9,70.86,279.89,218.45,8.64;9,70.86,291.84,219.93,8.64;9,70.86,303.80,28.50,8.64"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Agreement with human rationales in terms of mean Average Precision and False Positive Rate for Contribution Predictor (CP) and three alternative techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="13,510.79,701.97,13.63,9.46;13,306.14,715.52,220.09,9.46;13,306.14,728.72,140.71,9.81;13,447.16,726.76,3.46,6.99;13,451.12,729.07,3.03,9.46;13,365.95,747.31,17.18,12.32;13,379.04,755.13,2.88,6.99;13,366.90,763.79,15.28,10.71;13,387.85,757.45,8.49,9.57;13,400.56,747.31,17.18,12.32;13,413.65,755.13,2.88,6.99;13,401.51,763.79,15.28,10.71;13,422.46,757.45,8.49,9.57;13,437.13,750.07,5.45,9.57;13,435.17,763.79,8.89,10.71;13,446.94,747.31,17.18,12.32;13,460.03,755.13,2.88,6.99;13,447.59,765.54,11.52,9.57;13,459.42,763.79,3.46,6.99;13,459.11,771.07,4.36,6.99;13,506.25,757.80,18.18,9.46;14,70.52,74.70,218.62,9.46;14,70.86,87.90,170.11,9.81;14,241.27,85.95,3.46,6.99;14,240.97,93.23,4.36,6.99;14,249.86,88.25,39.27,9.46;14,70.86,101.45,57.28,9.81;14,128.44,99.50,3.46,6.99;14,132.40,101.80,2.73,9.46"><figDesc>13)By comparing Equations 12 and 13, these derivatives are related with a term of θ ℓ : training, we can compute the gradient w.r.t. the dummy variable θ ℓ d and then divide it by θ ℓ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,305.84,213.25,220.24,240.66"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters in each dataset; LR: Learning rate; BSZ: Batch size; MaxLen: Maximum Token Length tremely high amount at the end of the training to mimic a hard removal function (1e+5 &lt; λ). Hence, datasets with the same amount of training epochs have similar λ trends.</figDesc><table coords="14,330.75,336.60,169.07,117.31"><row><cell>Dataset</cell><cell>γ</cell><cell>ϕ</cell><cell>λ</cell></row><row><cell>SST-2</cell><cell cols="2">5e-3 5e-4</cell><cell>10 Epoch</cell></row><row><cell>IMDB</cell><cell cols="2">5e-3 5e-4</cell><cell>10 Epoch</cell></row><row><cell cols="3">HateXplain 5e-2 2e-2</cell><cell>50 Epoch</cell></row><row><cell>MRPC</cell><cell cols="3">3e-2 5e-2 10 × 3 Epoch</cell></row><row><cell>MNLI</cell><cell cols="2">5e-3 5e-4</cell><cell>50 Epoch</cell></row><row><cell>QNLI</cell><cell cols="2">5e-3 1e-4</cell><cell>10 Epoch</cell></row><row><cell cols="3">AG's News 1e-1 1e-1</cell><cell>10 Epoch</cell></row><row><cell>DBPedia</cell><cell cols="2">1e-1 1e-1</cell><cell>50 Epoch</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="5,322.28,754.95,202.14,8.06;5,306.14,765.20,185.86,7.77"><p>Since θ is not in the computational DAG, we employed a dummy variable inside the model. See Appendix B.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,87.00,723.57,186.57,7.77"><p>See the statistics of datasets inTable 5 in Appendix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="6,87.00,734.42,202.14,7.77;6,70.86,744.39,218.22,7.77"><p>Since some of the datasets were not used originally, we had to search the hyperparameters based on the given ranges.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="6,87.00,754.95,202.14,8.06;6,70.86,765.20,80.45,7.77"><p>Note that θ and η are trainable terms that are tuned by the model during training.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,70.86,93.55,219.93,8.64;10,81.77,104.33,207.37,8.82;10,81.77,115.29,207.37,8.59;10,81.44,126.25,209.35,8.82;10,81.77,137.39,192.07,8.64" xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying Attention Flow in Transformers</title>
		<author>
			<persName coords=""><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.385</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4190" to="4197" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct coords="10,70.86,158.25,218.28,8.64;10,81.77,169.21,207.37,8.64;10,81.77,180.17,207.36,8.64;10,81.77,190.95,207.36,8.82;10,81.46,201.91,66.32,8.59" xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient-Based Attribution Methods</title>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Enea</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cengiz</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28954-6_9</idno>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="169" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.86,222.94,219.93,8.64;10,81.77,233.90,207.37,8.64;10,81.77,244.86,209.02,8.64;10,81.77,255.64,207.37,8.82;10,81.46,266.60,207.67,8.59;10,81.10,277.56,208.20,8.82;10,81.77,288.70,108.50,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main">A Diagnostic Study of Explainability Techniques for Text Classification</title>
		<author>
			<persName coords=""><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.263</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3256" to="3274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.86,309.56,219.93,8.64;10,81.77,320.52,207.37,8.64;10,81.77,331.30,207.37,8.82;10,81.52,342.26,209.27,8.59;10,81.77,353.22,209.02,8.82;10,81.77,364.36,152.22,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating Fact Checking Explanations</title>
		<author>
			<persName coords=""><forename type="first">Pepa</forename><surname>Atanasova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.656</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7352" to="7364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.86,385.21,219.93,8.64;10,81.77,396.17,207.37,8.64;10,81.77,407.13,207.37,8.64;10,81.46,417.91,207.68,8.59;10,81.77,428.87,207.37,8.59;10,81.44,439.83,209.36,8.82;10,81.77,450.97,89.12,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main">The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?</title>
		<author>
			<persName coords=""><forename type="first">Jasmijn</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.blackboxnlp-1.14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="149" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.86,471.83,219.94,8.64;10,81.77,482.79,207.37,8.64;10,81.77,493.57,209.02,8.82;10,81.77,504.53,207.37,8.59;10,81.77,515.49,208.61,8.82;10,81.77,526.63,209.02,8.64;10,81.77,537.59,152.22,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main">Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience</title>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Chrysostomou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.645</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8189" to="8200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.86,558.44,218.28,8.64;10,81.77,569.40,207.37,8.64;10,81.77,580.36,209.03,8.64;10,81.77,591.14,207.37,8.82;10,81.77,602.10,207.37,8.59;10,81.44,613.06,209.36,8.59;10,81.77,624.02,207.36,8.82;10,81.42,635.16,207.88,8.64;10,81.77,646.12,108.50,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main"></title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North</title>
		<meeting>the 2019 Conference of the North<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="10,70.86,666.98,219.53,8.64;10,81.77,677.94,207.37,8.64;10,81.77,688.90,207.37,8.64;10,81.77,699.68,207.37,8.82;10,81.77,710.64,207.37,8.59;10,81.44,721.60,209.44,8.82;10,81.41,732.73,172.14,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main">ERASER: A Benchmark to Evaluate Rationalized NLP Models</title>
		<author>
			<persName coords=""><forename type="first">Jay</forename><surname>Deyoung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><forename type="middle">Fatema</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.408</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4443" to="4458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,70.86,753.59,219.93,8.64;10,81.77,764.55,209.12,8.64;10,317.05,75.14,207.37,8.82;10,317.05,86.10,115.96,8.59" xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing</meeting>
		<imprint>
			<publisher>IWP</publisher>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,106.99,219.93,8.64;10,317.05,117.95,209.02,8.64;10,317.05,128.91,207.37,8.64;10,316.74,139.69,207.68,8.59;10,316.64,150.65,207.79,8.59;10,317.05,161.61,209.02,8.59;10,317.05,172.56,208.62,8.82;10,317.05,183.70,207.37,8.64;10,317.05,194.66,46.78,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main">How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings</title>
		<author>
			<persName coords=""><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1006</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,215.37,219.52,8.64;10,317.05,227.48,207.36,8.64;10,317.05,238.44,209.03,8.64;10,317.05,249.22,159.01,8.82" xml:id="b10">
	<analytic>
		<title level="a" type="main">DACT-BERT: Differentiable Adaptive Computation Time for an Efficient BERT Inference</title>
		<author>
			<persName><forename type="first">Cristobal</forename><surname>Eyzaguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Del Rio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladimir</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Soto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.nlppower-1.10</idno>
		<idno type="arXiv">arXiv:2109.11745</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP</title>
		<meeting>NLP Power! The First Workshop on Efficient Benchmarking in NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="93" to="99" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,306.14,270.10,220.02,8.64;10,317.05,281.06,66.86,8.64;10,403.03,281.06,123.05,8.64;10,317.05,291.84,208.61,8.82;10,317.05,302.98,60.88,8.64" xml:id="b11">
	<monogr>
		<title level="m" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yunchao Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bourdev</surname></persName>
		</author>
		<idno>ArXiv, abs/1412.6115</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,323.69,218.28,8.64;10,317.05,334.65,208.62,8.64;10,317.05,345.61,207.37,8.64;10,317.05,356.57,209.02,8.64;10,317.05,367.35,209.02,8.82;10,317.05,378.31,122.33,8.82" xml:id="b12">
	<analytic>
		<title level="a" type="main">Power-bert: Accelerating bert inference via progressive word-vector elimination</title>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anamitra</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Raje</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Venkatesan</forename><surname>Chakaravarthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yogish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3690" to="3699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,399.20,218.28,8.64;10,317.05,410.15,207.37,8.64;10,317.05,421.11,207.36,8.64;10,316.80,431.89,209.27,8.59;10,317.05,442.85,209.12,8.59;10,316.80,453.81,149.65,8.59" xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. May 2-4, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,474.70,218.28,8.64;10,317.05,485.66,209.11,8.64;10,316.80,496.44,207.62,8.59;10,316.45,507.40,134.03,8.82" xml:id="b14">
	<analytic>
		<title level="a" type="main">Channel Pruning for Accelerating Very Deep Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.155</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">2017. 2017</date>
			<biblScope unit="page" from="1398" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,528.29,220.03,8.64;10,317.05,539.25,209.11,8.64;10,316.45,550.03,94.09,8.82" xml:id="b15">
	<monogr>
		<title level="m" type="main">Figure 2: The distinction between the conventional logits-based knowledge distillation (Hinton, Vinyals &amp; Dean, 2015) and TSKD proposed in this article.</title>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerjcs.1650/fig-2</idno>
		<idno>ArXiv, abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>PeerJ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,570.91,219.53,8.64;10,317.05,581.87,209.03,8.64;10,316.80,592.65,207.62,8.82;10,316.39,603.61,83.29,8.82" xml:id="b16">
	<analytic>
		<title level="a" type="main">Imitation Learning</title>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Hussein</surname></persName>
			<idno type="ORCID">0000-0001-5227-9929</idno>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohamed</forename><forename type="middle">Medhat</forename><surname>Gaber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eyad</forename><surname>Elyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chrisina</forename><surname>Jayne</surname></persName>
		</author>
		<idno type="DOI">10.1145/3054912</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<title level="j" type="abbrev">ACM Comput. Surv.</title>
		<idno type="ISSN">0360-0300</idno>
		<idno type="ISSNe">1557-7341</idno>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2017-04-06">2017</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,624.50,218.27,8.64;10,317.05,635.28,209.02,8.82;10,317.05,646.24,209.02,8.59;10,317.05,657.20,209.03,8.59;10,317.05,668.16,209.02,8.59;10,317.05,679.12,209.11,8.82;10,316.69,690.26,172.14,8.64" xml:id="b17">
	<analytic>
		<title level="a" type="main"></title>
		<author>
			<persName coords=""><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1357</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North</title>
		<meeting>the 2019 Conference of the North<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3543" to="3556" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="10,306.14,710.97,219.93,8.64;10,317.05,721.93,209.02,8.64;10,317.05,732.71,113.15,8.82" xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to Faithfully Rationalize by Construction</title>
		<author>
			<persName coords=""><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.409</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,306.14,753.59,218.28,8.64;10,317.05,764.55,209.11,8.64;11,81.46,75.32,209.33,8.64;11,81.77,86.10,209.02,8.82;11,81.77,97.06,208.86,8.82;11,81.42,108.20,209.37,8.64;11,81.77,119.16,32.94,8.64" xml:id="b19">
	<analytic>
		<title level="a" type="main">TinyBERT: Distilling BERT for Natural Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.372</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.86,142.47,219.53,8.64;11,81.77,153.43,207.37,8.64;11,81.77,164.39,209.03,8.64;11,81.77,175.17,207.37,8.82;11,81.77,186.13,209.02,8.59;11,81.77,197.09,209.03,8.59;11,81.41,208.05,209.38,8.59;11,81.77,219.00,209.02,8.82;11,81.77,230.14,161.07,8.64" xml:id="b20">
	<analytic>
		<title level="a" type="main">Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1023</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
	<note>Long Papers Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="11,70.86,253.45,219.93,8.64;11,81.77,264.41,208.61,8.64;11,81.77,275.19,207.37,8.82;11,81.16,286.15,207.98,8.59;11,81.49,297.11,209.31,8.59;11,81.77,308.07,209.02,8.59;11,81.49,319.03,207.65,8.82;11,81.77,330.17,122.61,8.64" xml:id="b21">
	<analytic>
		<title level="a" type="main">Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search</title>
		<author>
			<persName coords=""><forename type="first">Gyuwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.508</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6501" to="6511" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="11,70.86,353.48,218.28,8.64;11,81.52,364.44,209.27,8.64;11,81.77,375.40,207.37,8.64;11,81.41,386.18,207.72,8.82;11,81.77,397.14,207.37,8.82;11,81.42,408.28,207.72,8.64;11,81.77,419.24,46.78,8.64" xml:id="b22">
	<analytic>
		<title level="a" type="main">Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words</title>
		<author>
			<persName coords=""><forename type="first">Josef</forename><surname>Klafka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.434</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4801" to="4811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.86,442.55,219.52,8.64;11,81.77,453.51,207.37,8.64;11,81.77,464.47,208.61,8.64;11,81.77,475.43,209.02,8.64;11,81.77,486.39,209.11,8.64;11,81.52,497.17,116.78,8.82" xml:id="b23">
	<analytic>
		<title level="a" type="main">DBpedia – A large-scale, multilingual knowledge base extracted from Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<idno type="DOI">10.3233/sw-140134</idno>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<idno type="ISSN">1570-0844</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2015">2015</date>
			<publisher>SAGE Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.86,520.66,220.03,8.64;11,81.77,531.62,207.37,8.64;11,81.77,542.40,207.37,8.82;11,81.77,553.36,207.37,8.59;11,81.77,564.32,207.37,8.59;11,81.21,575.28,209.67,8.82;11,81.41,586.42,172.14,8.64" xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing and Understanding Neural Models in NLP</title>
		<author>
			<persName coords=""><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n16-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="681" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.86,609.73,220.02,8.64;11,81.41,620.69,207.73,8.64;11,81.77,631.47,207.37,8.82;11,81.16,642.43,207.98,8.82;11,81.02,653.56,209.77,8.64;11,81.77,664.52,79.16,8.64" xml:id="b25">
	<analytic>
		<title level="a" type="main">An End-to-End Multi-task Learning Model for Fact Checking</title>
		<author>
			<persName coords=""><forename type="first">Sizhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-5523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</title>
		<meeting>the First Workshop on Fact Extraction and VERification (FEVER)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="138" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,70.86,687.84,219.53,8.64;11,81.41,698.80,207.73,8.64;11,81.77,709.75,209.02,8.64;11,81.77,720.54,207.87,8.82;11,81.44,731.49,207.70,8.59;11,81.16,742.45,207.98,8.59;11,81.49,753.41,209.40,8.82;11,81.41,764.55,172.14,8.64" xml:id="b26">
	<analytic>
		<title level="a" type="main">A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models</title>
		<author>
			<persName coords=""><forename type="first">Kaiyuan</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bin</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2013" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,306.14,75.32,219.53,8.64;11,317.05,86.28,209.02,8.64;11,317.05,97.24,207.37,8.64;11,316.74,108.02,209.34,8.59;11,317.05,118.98,208.86,8.82;11,317.05,130.12,209.02,8.64;11,317.05,141.08,32.94,8.64" xml:id="b27">
	<analytic>
		<title level="a" type="main">FastBERT: a Self-distilling BERT with Adaptive Inference Time</title>
		<author>
			<persName coords=""><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><forename type="middle">I</forename><surname>Ju</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.537</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6035" to="6044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,306.14,159.59,218.28,8.64;11,316.69,170.37,209.38,8.82;11,317.05,181.33,138.55,8.59" xml:id="b28">
	<analytic>
		<title level="a" type="main">Second International Confer-ence on Integrable Systems and Nonlinear Dynamics - 2020</title>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.36684/28</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>Chechen State University</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,306.14,200.01,219.53,8.64;11,317.05,210.97,209.11,8.64;11,317.05,221.93,209.11,8.64;11,317.05,232.71,207.37,8.82;11,316.45,243.67,207.98,8.59;11,316.77,254.63,208.89,8.82;11,317.05,265.77,209.02,8.64;11,317.05,276.73,32.94,8.64" xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,306.14,295.24,220.03,8.64;11,317.05,306.20,209.02,8.64;11,316.69,316.98,207.73,8.82;11,316.69,327.94,209.38,8.59;11,317.05,338.90,31.82,8.59" xml:id="b30">
	<analytic>
		<title level="a" type="main">Building DNN acoustic models for large vocabulary speech recognition</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">T</forename><surname>Lengerich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csl.2016.06.007</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<title level="j" type="abbrev">Computer Speech &amp; Language</title>
		<idno type="ISSN">0885-2308</idno>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="195" to="213" />
			<date type="published" when="2013">2013</date>
			<publisher>Elsevier BV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,306.14,357.59,219.53,8.64;11,317.05,368.55,209.02,8.64;11,317.05,379.51,209.02,8.64;11,317.05,390.29,165.17,8.82" xml:id="b31">
	<analytic>
		<title level="a" type="main">HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection</title>
		<author>
			<persName coords=""><forename type="first">Binny</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Punyajoy</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seid</forename><forename type="middle">Muhie</forename><surname>Yimam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v35i17.17745</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="14867" to="14875" />
			<date type="published" when="2021-05-18">2021</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,306.14,408.98,220.03,8.64;11,316.69,419.76,209.47,8.82" xml:id="b32">
	<monogr>
		<title level="m" type="main">Are sixteen heads really better than one? In NeurIPS</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,306.14,438.45,219.93,8.64;11,317.05,449.40,207.37,8.64;11,317.05,460.36,209.02,8.64;11,317.05,471.14,207.36,8.82;11,316.74,482.10,208.92,8.59;11,317.05,493.24,207.37,8.64;11,317.05,504.20,209.03,8.64" xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring the Role of BERT Token Representations to Explain Sentence Probing Results</title>
		<author>
			<persName coords=""><forename type="first">Hosein</forename><surname>Mohebbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Modarressi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.61</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="792" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,306.14,522.71,220.03,8.64;11,317.05,533.67,207.36,8.64;11,317.05,544.45,207.37,8.82;11,316.72,555.41,209.35,8.59;11,317.05,566.37,208.61,8.59;11,317.05,577.51,209.02,8.64;11,317.05,588.47,71.96,8.64" xml:id="b34">
	<analytic>
		<title level="a" type="main">Telling BERT’s Full Story: from Local Attention to Global Aggregation</title>
		<author>
			<persName coords=""><forename type="first">Damian</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gino</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.9</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="105" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,306.14,606.98,218.27,8.64;11,317.05,617.94,207.54,8.64;11,317.05,628.72,207.37,8.82;11,317.05,639.68,209.02,8.59;11,317.05,650.64,208.62,8.82;11,316.74,661.78,200.07,8.64" xml:id="b35">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,306.14,680.29,218.28,8.64;11,316.74,691.24,207.68,8.64;11,317.05,702.03,207.37,8.82;11,317.05,712.98,110.05,8.59" xml:id="b36">
	<analytic>
		<title level="a" type="main">Smaller faster lighter denser cheaper: how innovation keeps proving the catastrophists wrong</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.5860/choice.169427</idno>
		<idno type="arXiv">arXiv:1910.01108</idno>
	</analytic>
	<monogr>
		<title level="j">Choice Reviews Online</title>
		<title level="j" type="abbrev">Choice Reviews Online</title>
		<idno type="ISSN">0009-4978</idno>
		<idno type="ISSNe">1523-8253</idno>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page">52-1378-52-1378</biblScope>
			<date type="published" when="2019">2019</date>
			<publisher>American Library Association</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,306.14,731.67,220.02,8.64;11,317.05,742.63,209.11,8.64;11,316.45,753.41,209.22,8.59;11,317.05,764.55,70.02,8.64" xml:id="b37">
	<analytic>
		<title level="a" type="main">Movement pruning: Adaptive sparsity by fine-tuning</title>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20378" to="20389" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.86,75.32,218.28,8.64;12,81.77,86.10,207.36,8.82;12,81.16,97.06,70.83,8.82" xml:id="b38">
	<analytic>
		<title level="a" type="main">Green AI</title>
		<author>
			<persName coords=""><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno type="DOI">10.1145/3381831</idno>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<title level="j" type="abbrev">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<idno type="ISSNe">1557-7317</idno>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2020-11-17">2020</date>
			<publisher>Association for Computing Machinery (ACM)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.86,115.75,218.28,8.64;12,81.77,126.71,209.11,8.64;12,81.77,137.67,207.37,8.64;12,81.77,148.45,207.37,8.82;12,81.77,159.41,207.37,8.59;12,81.77,170.37,208.62,8.82;12,81.77,181.50,204.24,8.64" xml:id="b39">
	<analytic>
		<title level="a" type="main">The Right Tool for the Job: Matching Model and Instance Complexities</title>
		<author>
			<persName coords=""><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.593</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6640" to="6651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.86,200.01,219.93,8.64;12,81.77,210.79,209.02,8.82;12,81.77,221.75,208.61,8.59;12,81.77,232.89,207.54,8.64;12,81.77,243.85,108.50,8.64" xml:id="b40">
	<analytic>
		<title level="a" type="main">Is Attention Interpretable?</title>
		<author>
			<persName coords=""><forename type="first">Sofia</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1282</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2931" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.86,262.36,218.28,8.64;12,81.19,273.32,207.95,8.64;12,81.77,284.28,209.02,8.64;12,81.77,295.06,145.26,8.82" xml:id="b41">
	<analytic>
		<title level="a" type="main">Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT</title>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiayu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Linjian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6409</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<title level="j" type="abbrev">AAAI</title>
		<idno type="ISSN">2159-5399</idno>
		<idno type="ISSNe">2374-3468</idno>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="8815" to="8821" />
			<date type="published" when="2020-04-03">2020</date>
			<publisher>Association for the Advancement of Artificial Intelligence (AAAI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.86,313.75,219.93,8.64;12,81.77,324.71,208.75,8.64;12,81.41,335.67,208.08,8.64;12,81.77,346.45,156.81,8.82" xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,70.86,365.14,219.93,8.64;12,81.77,376.10,207.37,8.64;12,81.77,386.88,184.68,8.82" xml:id="b43">
	<monogr>
		<title level="m" type="main">Smoothgrad: removing noise by adding noise</title>
		<author>
			<persName coords=""><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wattenberg</surname></persName>
		</author>
		<idno>arxiv:1706.03825</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arxiv</note>
</biblStruct>

<biblStruct coords="12,70.86,405.57,218.28,8.64;12,81.77,416.53,207.37,8.64;12,81.77,427.49,207.54,8.64;12,81.77,438.45,209.11,8.64;12,81.77,449.23,209.02,8.82;12,81.77,460.18,207.36,8.82;12,81.02,471.32,208.12,8.64;12,81.77,482.28,122.61,8.64" xml:id="b44">
	<analytic>
		<title level="a" type="main">Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d13-1170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.86,500.79,220.02,8.64;12,81.77,511.75,209.03,8.64;12,81.77,522.53,207.37,8.82;12,81.46,533.49,207.67,8.59;12,81.77,544.45,209.02,8.59;12,81.77,555.41,207.37,8.82;12,81.42,566.55,209.37,8.64;12,81.77,577.51,89.12,8.64" xml:id="b45">
	<analytic>
		<title level="a" type="main">Patient Knowledge Distillation for BERT Model Compression</title>
		<author>
			<persName coords=""><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1441</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4323" to="4332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.86,596.02,218.28,8.64;12,81.77,606.98,207.37,8.64;12,81.41,617.94,209.39,8.64;12,81.77,628.72,194.72,8.82" xml:id="b46">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunhua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhao</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13792</idno>
		<title level="m">Early exiting with ensemble internal classifiers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,70.86,647.41,219.52,8.64;12,81.19,658.37,209.33,8.64;12,81.77,669.33,207.37,8.64;12,81.77,680.11,209.02,8.82;12,81.77,691.07,208.61,8.59;12,81.77,702.20,209.02,8.64;12,81.77,713.16,71.96,8.64" xml:id="b47">
	<analytic>
		<title level="a" type="main">MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</title>
		<author>
			<persName coords=""><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.195</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2158" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,70.86,731.67,220.02,8.64;12,81.41,742.45,209.38,8.82;12,81.77,753.41,207.36,8.59;12,81.49,764.37,159.12,8.82" xml:id="b48">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName coords=""><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,306.14,75.32,219.53,8.64;12,316.74,86.28,207.85,8.64;12,317.05,97.24,208.61,8.64;12,317.05,108.20,209.02,8.64;12,317.05,119.16,209.02,8.64;12,317.05,129.94,207.37,8.82;12,316.45,140.90,209.63,8.59;12,317.05,151.86,63.14,8.59" xml:id="b49">
	<analytic>
		<title level="a" type="main">Edgebert: Sentence-level energy optimizations for latencyaware multi-task nlp inference</title>
		<author>
			<persName coords=""><forename type="first">Coleman</forename><surname>Thierry Tambe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lillian</forename><surname>Hooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianyu</forename><surname>Pentecost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">En-Yu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gu-Yeon</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,306.14,172.22,218.28,8.64;12,317.05,183.18,207.37,8.64;12,317.05,194.13,207.37,8.64;12,316.80,204.91,209.27,8.82;12,317.05,215.87,112.98,8.82" xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,432.53,216.05,92.97,8.64" xml:id="b51">
	<monogr>
		<title level="m" type="main">Feasibility study for low-head hydropower on the Mill River, Northampton, Massachusetts</title>
		<idno type="DOI">10.2172/5766688</idno>
		<imprint>
			<date type="published" when="1979-01-01" />
			<publisher>Office of Scientific and Technical Information (OSTI)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,306.14,236.23,218.53,8.64;12,317.05,247.19,208.75,8.64;12,316.69,258.15,209.39,8.64;12,317.05,268.93,207.37,8.82;12,316.80,279.89,207.62,8.59;12,317.05,290.85,207.37,8.82;12,317.05,301.99,209.02,8.64;12,317.05,312.95,89.12,8.64" xml:id="b52">
	<analytic>
		<title level="a" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,306.14,333.13,219.93,8.64;12,317.05,344.09,209.02,8.64;12,317.05,354.87,209.03,8.82;12,317.05,365.83,207.37,8.59;12,316.45,376.79,146.60,8.82" xml:id="b53">
	<analytic>
		<title level="a" type="main">SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning</title>
		<author>
			<persName coords=""><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/hpca51647.2021.00018</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-02">2021. 2021</date>
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,306.14,397.15,220.02,8.64;12,317.05,408.11,209.02,8.64;12,317.05,418.89,209.03,8.82;12,317.05,429.85,207.37,8.59;12,316.72,440.81,209.36,8.59;12,317.05,451.77,207.37,8.59;12,316.31,462.73,209.36,8.82;12,317.05,473.86,209.02,8.64;12,317.05,484.82,16.33,8.64" xml:id="b54">
	<analytic>
		<title level="a" type="main">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</title>
		<author>
			<persName coords=""><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="12,306.14,505.00,218.28,8.64;12,317.05,515.96,209.03,8.64;12,317.05,526.92,209.02,8.64;12,317.05,537.88,208.61,8.64;12,317.05,548.84,208.61,8.64;12,316.74,559.80,208.92,8.64;12,317.05,570.76,209.02,8.64;12,317.05,581.72,209.11,8.64;12,317.05,592.50,207.37,8.82;12,316.64,603.46,207.79,8.59;12,316.69,614.42,207.73,8.82;12,317.05,625.56,122.61,8.64" xml:id="b55">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,306.14,645.74,218.28,8.64;12,316.86,656.70,207.56,8.64;12,317.05,667.48,207.37,8.82;12,317.05,678.44,207.37,8.59;12,316.72,689.40,209.44,8.82;12,316.69,700.53,172.14,8.64" xml:id="b56">
	<analytic>
		<title level="a" type="main">DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference</title>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaejun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.204</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2246" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,306.14,720.71,220.03,8.64;12,317.05,731.67,207.54,8.64;12,317.05,742.45,209.02,8.82;12,317.05,753.41,209.02,8.59;12,317.05,764.37,209.03,8.59;13,81.35,75.14,207.95,8.82;13,81.77,86.28,108.50,8.64" xml:id="b57">
	<analytic>
		<title level="a" type="main">BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression</title>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="91" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.86,107.09,219.53,8.64;13,81.77,118.05,209.02,8.64;13,81.77,129.01,207.37,8.64;13,81.46,139.79,207.68,8.59;13,81.35,150.75,209.03,8.59;13,81.77,161.89,209.02,8.64;13,81.77,172.85,71.96,8.64" xml:id="b58">
	<analytic>
		<title level="a" type="main">BERT-of-Theseus: Compressing BERT by Progressive Module Replacing</title>
		<author>
			<persName coords=""><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.633</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7859" to="7869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.86,193.66,218.28,8.64;13,81.77,204.62,207.37,8.64;13,81.77,215.40,207.37,8.82;13,81.77,226.36,209.02,8.59;13,81.77,237.32,209.03,8.59;13,81.41,248.28,208.98,8.82;13,81.77,259.42,204.24,8.64" xml:id="b59">
	<analytic>
		<title level="a" type="main">TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference</title>
		<author>
			<persName coords=""><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yufei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.463</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5798" to="5809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.86,280.23,220.02,8.64;13,81.77,291.19,207.37,8.64;13,81.77,301.97,209.02,8.82;13,81.77,312.93,208.61,8.59;13,81.52,324.07,119.63,8.64" xml:id="b60">
	<analytic>
		<title level="a" type="main">Interpreting deep models for text analysis via optimization and regularization methods</title>
		<author>
			<persName coords=""><forename type="first">Yongjun</forename><surname>Hao Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuiwang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5717" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.86,344.89,219.93,8.64;13,81.77,355.85,207.37,8.64;13,81.77,366.63,207.36,8.82;13,81.44,377.58,209.35,8.59;13,81.77,388.54,209.11,8.82;13,81.41,399.68,172.14,8.64" xml:id="b61">
	<analytic>
		<title level="a" type="main">Modeling annotators</title>
		<author>
			<persName coords=""><forename type="first">Omar</forename><forename type="middle">F</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.3115/1613715.1613721</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing - EMNLP &apos;08<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.86,420.50,220.02,8.64;13,81.77,431.46,209.03,8.64;13,81.77,442.24,76.92,8.82" xml:id="b62">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.86,463.23,218.28,8.64;13,81.77,474.19,209.11,8.64;13,81.46,485.15,209.33,8.64;13,81.77,495.93,207.37,8.82;13,81.49,506.89,105.35,8.59" xml:id="b63">
	<analytic>
		<title level="a" type="main">Transformer-xh: Multi-evidence reasoning with extra hop attention</title>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,70.86,527.88,218.28,8.64;13,81.77,538.84,207.37,8.64;13,81.77,549.80,207.36,8.64;13,81.16,560.58,209.22,8.59;13,81.52,571.72,130.55,8.64" xml:id="b64">
	<analytic>
		<title level="a" type="main">Bert loses patience: Fast and robust inference with early exit</title>
		<author>
			<persName coords=""><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18330" to="18341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,214.54,571.72,75.83,8.64;13,81.77,582.68,15.21,8.64;15,116.78,209.68,109.01,8.27;15,95.94,355.57,15.84,6.79;15,133.55,355.57,67.36,6.79;15,81.79,369.06,18.61,5.09" xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-label learning with label-specific feature selection via local positive and negative correlation</title>
		<author>
			<persName><forename type="first">Xia</forename><surname>Shuxin</surname></persName>
			<idno type="ORCID">0009-0005-9966-7634</idno>
		</author>
		<idno type="DOI">10.36227/techrxiv.171470462.24905388/v1</idno>
	</analytic>
	<monogr>
		<title level="j">Sports Layer</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2024-05-03" />
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,375.49,19.91,5.09" xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,381.92,19.91,5.09" xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,388.35,20.00,5.09" xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,394.78,19.91,5.09" xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,401.22,19.95,5.09" xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,407.65,19.59,5.09" xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,414.08,20.05,5.09" xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,420.51,19.94,5.09" xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,426.94,21.99,5.09" xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,433.37,20.37,5.09" xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,81.79,439.80,21.68,5.09" xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Layer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
